{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리한 학습 데이터를 불러옴\n",
    "DATA_IN_PATH  = './data/'\n",
    "DATA_OUT_PATH = './submission/'\n",
    "\n",
    "TRAIN_INPUT_DATA = 'train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs_en.json'\n",
    "\n",
    "train_input = np.load(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'rb'))\n",
    "train_label = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'rb'))\n",
    "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size 설정을 위해 검증 데이터를 불러옴\n",
    "# test_inputs.npy에 prepro_configs['vocab_size']의 범위([0, 74067))를 넘는 데이터가 존재하기 때문\n",
    "TEST_INPUT_DATA = 'test_inputs.npy'\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "SAVE_FILE_NAME = 'weights.h5'\n",
    "\n",
    "test_input = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))\n",
    "test_input = pad_sequences(test_input, maxlen=test_input.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 고정\n",
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터 정의\n",
    "model_name = 'cnn_classifier_en'\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "MAX_SENTENCE_LENGTH = np.max(test_input) + 1\n",
    "\n",
    "kargs = {'model_name': model_name,\n",
    "        'vocab_size': MAX_SENTENCE_LENGTH,\n",
    "        'embedding_dimension': 128,\n",
    "        'num_filters': 100,\n",
    "        'dropout_rate': 0.5,\n",
    "        'hidden_dimension': 100,\n",
    "        'output_dimension': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구현\n",
    "class CNNClassifier(Model):\n",
    "    def __init__(self, **kargs):\n",
    "        super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "        self.embedding = Embedding(input_dim=kargs['vocab_size'],\n",
    "                                  output_dim=kargs['embedding_dimension'])\n",
    "        self.conv_list = [Conv1D(filters=kargs['num_filters'],\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding='valid',\n",
    "                                activation='relu',\n",
    "                                kernel_constraint=MaxNorm(max_value=3.))\n",
    "                         for kernel_size in [3,4,5]]\n",
    "        self.max_pooling = GlobalMaxPooling1D()\n",
    "        self.dropout = Dropout(kargs['dropout_rate'])\n",
    "        self.fc1 = Dense(units=kargs['hidden_dimension'],\n",
    "                        activation='relu', kernel_constraint=MaxNorm(max_value=3.))\n",
    "        self.fc2 = Dense(units=kargs['output_dimension'],\n",
    "                        activation='sigmoid', kernel_constraint=MaxNorm(max_value=3.))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.concat([self.max_pooling(conv(x)) for conv in self.conv_list], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = CNNClassifier(**kargs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./submission/cnn_classifier -- Folder already exists \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfitting 방지\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=1)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\n",
    "\n",
    "checkpoint_path = DATA_OUT_PATH + model_name + '/weights.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "\n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9215\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.87720, saving model to ./submission/cnn_classifier/weights.h5\n",
      "352/352 [==============================] - 63s 178ms/step - loss: 0.1990 - accuracy: 0.9215 - val_loss: 0.3141 - val_accuracy: 0.8772\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9744\n",
      "Epoch 00002: val_accuracy improved from 0.87720 to 0.88040, saving model to ./submission/cnn_classifier/weights.h5\n",
      "352/352 [==============================] - 61s 173ms/step - loss: 0.0742 - accuracy: 0.9744 - val_loss: 0.3653 - val_accuracy: 0.8804\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9884\n",
      "Epoch 00003: val_accuracy did not improve from 0.88040\n",
      "352/352 [==============================] - 59s 167ms/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.4376 - val_accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                   validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 검증 점수가 나온 모델을 불러옴\n",
    "model.load_weights(os.path.join(DATA_OUT_PATH, model_name, SAVE_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle에 제출할 CSV파일을 만들어 저장\n",
    "predictions = model.predict(test_input)\n",
    "predictions = predictions.squeeze(-1)\n",
    "\n",
    "test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'), allow_pickle=True)\n",
    "\n",
    "output = pd.DataFrame(data={'id': list(test_id), 'sentiment': list(predictions)})\n",
    "output.to_csv(DATA_OUT_PATH + 'movie_review_sentiment_analysis_CNN.csv', index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
