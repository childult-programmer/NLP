{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec을 활용한 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력값은 단어로 표현된 리스트\n",
    "DATA_IN_PATH = './data/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "# 전처리된 텍스트 데이터를 불러옴\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])\n",
    "\n",
    "# 단어들의 리스트로 나눔\n",
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec을 학습하는 진행 상황을 확인해보기 위해 logging을 이용\n",
    "import logging\n",
    "\n",
    "# 학습 과정에서 로그 메시지를 양식에 맞게 INFO 수준으로 보여줌\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-31 22:41:25,162 : INFO : collecting all words and their counts\n",
      "2020-10-31 22:41:25,162 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-31 22:41:25,518 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2020-10-31 22:41:25,880 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2020-10-31 22:41:26,058 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2020-10-31 22:41:26,059 : INFO : Loading a fresh vocabulary\n",
      "2020-10-31 22:41:26,113 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2020-10-31 22:41:26,114 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2020-10-31 22:41:26,146 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2020-10-31 22:41:26,148 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2020-10-31 22:41:26,149 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2020-10-31 22:41:26,170 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2020-10-31 22:41:26,171 : INFO : resetting layer weights\n",
      "2020-10-31 22:41:28,395 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-10-31 22:41:29,402 : INFO : EPOCH 1 - PROGRESS: at 49.68% examples, 1243422 words/s, in_qsize 7, out_qsize 0\n",
      "2020-10-31 22:41:30,403 : INFO : EPOCH 1 - PROGRESS: at 98.46% examples, 1226022 words/s, in_qsize 5, out_qsize 0\n",
      "2020-10-31 22:41:30,416 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-31 22:41:30,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-31 22:41:30,423 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-31 22:41:30,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-31 22:41:30,427 : INFO : EPOCH - 1 : training on 2988089 raw words (2494449 effective words) took 2.0s, 1228999 effective words/s\n",
      "2020-10-31 22:41:31,435 : INFO : EPOCH 2 - PROGRESS: at 51.68% examples, 1295109 words/s, in_qsize 7, out_qsize 0\n",
      "2020-10-31 22:41:32,375 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-31 22:41:32,378 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-31 22:41:32,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-31 22:41:32,389 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-31 22:41:32,390 : INFO : EPOCH - 2 : training on 2988089 raw words (2494577 effective words) took 2.0s, 1274140 effective words/s\n",
      "2020-10-31 22:41:33,398 : INFO : EPOCH 3 - PROGRESS: at 48.39% examples, 1212923 words/s, in_qsize 7, out_qsize 0\n",
      "2020-10-31 22:41:34,401 : INFO : EPOCH 3 - PROGRESS: at 96.76% examples, 1205164 words/s, in_qsize 7, out_qsize 0\n",
      "2020-10-31 22:41:34,462 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-31 22:41:34,464 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-31 22:41:34,468 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-31 22:41:34,478 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-31 22:41:34,479 : INFO : EPOCH - 3 : training on 2988089 raw words (2494354 effective words) took 2.1s, 1197060 effective words/s\n",
      "2020-10-31 22:41:35,504 : INFO : EPOCH 4 - PROGRESS: at 43.41% examples, 1070779 words/s, in_qsize 7, out_qsize 0\n",
      "2020-10-31 22:41:36,504 : INFO : EPOCH 4 - PROGRESS: at 92.75% examples, 1147582 words/s, in_qsize 7, out_qsize 0\n",
      "2020-10-31 22:41:36,628 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-31 22:41:36,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-31 22:41:36,635 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-31 22:41:36,643 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-31 22:41:36,643 : INFO : EPOCH - 4 : training on 2988089 raw words (2494119 effective words) took 2.2s, 1155428 effective words/s\n",
      "2020-10-31 22:41:37,652 : INFO : EPOCH 5 - PROGRESS: at 49.68% examples, 1242813 words/s, in_qsize 7, out_qsize 0\n",
      "2020-10-31 22:41:38,657 : INFO : EPOCH 5 - PROGRESS: at 97.78% examples, 1214647 words/s, in_qsize 7, out_qsize 0\n",
      "2020-10-31 22:41:38,692 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-31 22:41:38,693 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-31 22:41:38,701 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-31 22:41:38,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-31 22:41:38,713 : INFO : EPOCH - 5 : training on 2988089 raw words (2494154 effective words) took 2.1s, 1207162 effective words/s\n",
      "2020-10-31 22:41:38,714 : INFO : training on a 14940445 raw words (12471653 effective words) took 10.3s, 1208662 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# gensim 라이브러리의 word2vec을 활용해 모델 생성 및 학습을 실행\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 학습 시 필요한 하이퍼파라미터를 설정\n",
    "num_features = 300     # 워드 벡터 특징값 수\n",
    "min_word_count = 40    # 단어에 대한 최소 빈도 수\n",
    "num_workers = 4        # 프로세스 개수\n",
    "context = 10           # 컨텍스트 윈도 크기\n",
    "downsampling = 1e-3    # 다움 샘플링 비율, 보통 0.001이 좋은 성능을 낸다고 알려져 있음\n",
    "\n",
    "print('Training model ...')\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers = num_workers,\n",
    "                         size = num_features,\n",
    "                         min_count = min_word_count,\n",
    "                         window = context,\n",
    "                         sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-31 22:41:38,720 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2020-10-31 22:41:38,722 : INFO : not storing attribute vectors_norm\n",
      "2020-10-31 22:41:38,722 : INFO : not storing attribute cum_table\n",
      "2020-10-31 22:41:38,904 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담아 저장하면 나중에 참고하기 좋으며\n",
    "# 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있음\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_features(words, model, num_features):\n",
    "    # words : 단어의 모음인 하나의 리뷰\n",
    "    # model : 이미 학습한 word2cec 모델\n",
    "    # num_features : word2vec으로 임베딩할 때 정했던 벡터의 차원 수\n",
    "    \n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘사전 준비\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vecotr = np.add(feature_vector, model[w])\n",
    "            \n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    # reviews : 학습 데이터인 전체 리뷰 데이터\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "    \n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dataset()을 이용해 실제 학습에 사용될 입력값을 생성\n",
    "test_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습과 검증 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "X = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 선언 및 학습\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.496200\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터를 이용한 모델 성능 평가\n",
    "print('Accuracy: {:f}'.format(lgs.score(X_eval, y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle에 제출할 데이터 생성\n",
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_review = list(test_data['review'])\n",
    "\n",
    "test_sentences = []\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터들을 word2vec으로 임베딩된 벡터값을 갖게 함\n",
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle에 제출할 CSV파일을 만들어 저장\n",
    "DATA_OUT_PATH = './submission/'\n",
    "\n",
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "    \n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_word2vec_submission.csv', index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
